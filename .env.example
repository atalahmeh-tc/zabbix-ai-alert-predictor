# .env.example
# Copy this file to .env and update with your actual configurations


# Ollama Configuration
# For local development (when running outside Docker)
AI_HOST=http://localhost:11434
AI_MODEL=llama3.1:8b
AI_TEMPERATURE=0.2

# Zabbix Configuration
ZABBIX_URL=https://your-zabbix-server.com/api_jsonrpc.php
ZABBIX_USERNAME=your_username
ZABBIX_PASSWORD=your_password
